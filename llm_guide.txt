GUÍA PARA CAMBIAR EL MODELO LLM
==============================

1. MODELOS RECOMENDADOS Y COMPARATIVAS
------------------------------------

MODELOS PEQUEÑOS (2-4GB RAM)
----------------------------
- facebook/opt-125m (muy ligero, básico)
- microsoft/phi-1.5 (≈ GPT-3 pequeño)
- google/flan-t5-small (excelente para Q&A)
- EleutherAI/pythia-160m (bueno para código)

MODELOS MEDIANOS (4-8GB RAM)
---------------------------
- TinyLlama/TinyLlama-1.1B-Chat-v1.0 (≈ GPT-3.5 pequeño)
- facebook/opt-1.3b (buena comprensión)
- google/flan-t5-base (versátil)
- microsoft/phi-2 (≈ GPT-3.5 mediano)

MODELOS GRANDES (8GB+ RAM)
-------------------------
LLAMA 3:
- meta-llama/Llama-3-8b-hf (≈ GPT-3.5)
- meta-llama/Llama-3-13b-hf (≈ GPT-3.5-turbo)
- meta-llama/Llama-3-70b-hf (≈ GPT-4)

LLAMA 2:
- meta-llama/Llama-2-7b-chat-hf (≈ GPT-3.5)
- meta-llama/Llama-2-13b-chat-hf (≈ GPT-3.5-turbo)
- meta-llama/Llama-2-70b-chat-hf (≈ GPT-4)

OTROS GRANDES:
- mistralai/Mixtral-8x7B-v0.1 (≈ GPT-4)
- mistralai/Mistral-7B-v0.1 (≈ GPT-3.5-turbo)
- Intel/neural-chat-7b-v3-1 (≈ GPT-3.5)

COMPARATIVA CON OPENAI
---------------------
Equivalentes a GPT-3.5:
- meta-llama/Llama-3-8b-hf
- mistralai/Mistral-7B-v0.1
- Intel/neural-chat-7b-v3-1

Equivalentes a GPT-3.5-turbo:
- meta-llama/Llama-3-13b-hf
- meta-llama/Llama-2-13b-chat-hf
- HuggingFaceH4/zephyr-7b-beta

Equivalentes a GPT-4:
- meta-llama/Llama-3-70b-hf
- meta-llama/Llama-2-70b-chat-hf
- mistralai/Mixtral-8x7B-v0.1

MODELOS ESPECIALIZADOS
---------------------
Para Código (≈ GitHub Copilot):
- bigcode/starcoder2-15b
- WizardLM/WizardCoder-Python-34B-V1.0
- Phind/Phind-CodeLlama-34B-v2

Para Chat:
- anthropic/claude-2-100k (≈ GPT-4)
- HuggingFaceH4/zephyr-7b-beta (≈ GPT-3.5-turbo)
- microsoft/phi-2 (compacto pero potente)

Para Q&A:
- google/flan-t5-xl (específico para Q&A)
- lmsys/fastchat-t5-3b-v1.0 (equilibrado)
- databricks/dolly-v2-3b (específico para Q&A)

2. CÓMO CAMBIAR EL MODELO
------------------------

Para cambiar el modelo LLM, sigue estos pasos:

1. Abre el archivo src/llm/model_manager.py

2. Localiza la línea donde se define el modelo por defecto:
   def __init__(self, model_name: str = "mistralai/Mistral-7B-v0.1"):

3. Reemplaza el nombre del modelo con cualquiera de los listados arriba.
   Ejemplo:
   def __init__(self, model_name: str = "microsoft/phi-2"):

4. Guarda el archivo y reinicia la aplicación.

NOTAS IMPORTANTES:
-----------------
1. Requisitos de Hardware:
   - Modelos pequeños: mínimo 4GB RAM
   - Modelos medianos: mínimo 8GB RAM
   - Modelos grandes: mínimo 16GB RAM y GPU recomendada

2. Primera Ejecución:
   - La primera vez que uses un modelo nuevo, se descargará automáticamente
   - El tiempo de descarga depende del tamaño del modelo
   - Asegúrate de tener espacio suficiente en disco

3. Optimización:
   - Para GPUs con poca memoria, usa modelos pequeños o medianos
   - Los modelos cuantitativos (4-bit, 8-bit) usan menos memoria
   - Ajusta max_length en generate_response() según necesites

4. Problemas Comunes:
   - "CUDA out of memory": Elige un modelo más pequeño
   - "Token not found": Actualiza huggingface-hub
   - "Model not found": Verifica el nombre del modelo y tu conexión

5. Recomendaciones:
   - Comienza con modelos pequeños y escala según necesidad
   - Prueba varios modelos para encontrar el mejor para tu caso
   - Mantén actualizada la librería transformers</content>